{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641bd699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain langgraph langchain_community langchain_openai langchain_core ollama pandas duckdb faiss-cpu sentence-transformers biopython pypdf pydantic lxml html2text beautifulsoup4 matplotlib -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc91cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if \"LANGCHAIN_API_KEY\" not in os.environ or \"ENTREZ_EMAIL\" not in os.environ:\n",
    "    print(\"Required environment variables not set. Please set them in your .env file or environment.\")\n",
    "else:\n",
    "    print(\"Environment variables loaded successfully.\")\n",
    "\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"AI_Clinical_Trials_Architect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630b7d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "llm_config = {\n",
    "    \"planner\": ChatOllama(model=\"llama3.1:8b\", temperature=0.0, format='json'),\n",
    "    \"drafter\": ChatOllama(model=\"qwen2:7b\", temperature=0.2),\n",
    "    \"sql_coder\": ChatOllama(model=\"qwen2:7b\", temperature=0.0),\n",
    "    \"director\": ChatOllama(model=\"llama3:70b\", temperature=0.0, format='json'),\n",
    "    \"embedding_model\": OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180eb1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LLM clients configured:\")\n",
    "print(f\"Planner ({llm_config['planner'].model}): {llm_config['planner']}\")\n",
    "print(f\"Drafter ({llm_config['drafter'].model}): {llm_config['drafter']}\")\n",
    "print(f\"SQL Coder ({llm_config['sql_coder'].model}): {llm_config['sql_coder']}\")\n",
    "print(f\"Director ({llm_config['director'].model}): {llm_config['director']}\")\n",
    "print(f\"Embedding Model ({llm_config['embedding_model'].model}): {llm_config['embedding_model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f66866",
   "metadata": {},
   "outputs": [],
   "source": [
    "global data_paths\n",
    "\n",
    "data_paths = {\n",
    "    \"base\": \"./data\",\n",
    "    \"pubmed\": \"./data/pubmed_articles\",\n",
    "    \"fda\": \"./data/fda_guidelines\",\n",
    "    \"ethics\": \"./data/ethical_guidelines\",\n",
    "    \"mimic\": \"./data/mimic_db\"\n",
    "}\n",
    "\n",
    "for path in data_paths.values():\n",
    "    # os.makedirs(path, exist_ok=True)\n",
    "    # print(f\"Directory initialized: {path}\")\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print(f\"Created directory: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3379ae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez, Medline\n",
    "\n",
    "def download_pubmed_articles(query, max_articles=20):\n",
    "    \"\"\"Fetches abstracts from PubMed for a given query and saves them as text files.\"\"\"\n",
    "\n",
    "    Entrez.email = os.environ.get(\"ENTREZ_EMAIL\", \"aashishjain3009@gmail.com\")\n",
    "\n",
    "    # Step 1: Use Entrez.esearch to find the PubMed IDs (PMIDs) for articles matching our query.\n",
    "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_articles, sort=\"relevance\")\n",
    "    record = Entrez.read(handle)\n",
    "    id_list = record[\"IdList\"]\n",
    "\n",
    "    # Step 2: Use Entrez.efetch to retrieve the full records (in MEDLINE format) for the list of PMIDs.\n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=id_list, rettype=\"medline\", retmode=\"text\")\n",
    "    records = Medline.parse(handle)\n",
    "\n",
    "    count = 0\n",
    "    # Step 3: Iterate through the retrieved records, parse them, and save each abstract to a file.\n",
    "    for i, record in enumerate(records):\n",
    "        pmid = record.get(\"PMID\", \"\")\n",
    "        title = record.get(\"TI\", \"No Title\")\n",
    "        abstract = record.get(\"AB\", \"No Abstract\")\n",
    "        if pmid:\n",
    "            filepath = os.path.join(data_paths[\"pubmed\"], f\"{pmid}.txt\")\n",
    "            with open(filepath, \"w\") as f:\n",
    "                f.write(f\"Title: {title}\\n\\nAbstract: {abstract}\")\n",
    "            print(f\"[{i+1}/{len(id_list)}] Fetching PMID: {pmid}... Saved to {filepath}\")\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75b0658",
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_query = \"(SGLT2 inhibitor) AND (type 2 diabetes) AND (renal impairment)\"\n",
    "num_downloaded = download_pubmed_articles(pubmed_query)\n",
    "print(f\"PubMed download complete. {num_downloaded} articles saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ad4977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, io\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def download_and_extract_text_from_pdf(url, output_path):\n",
    "    \"\"\"Downloads a PDF from a URL, saves it, and also extracts its text content to a separate .txt file.\"\"\"\n",
    "    print(f\"Downloading FDA Guideline: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Successfully downloaded and saved to {output_path}\")\n",
    "\n",
    "        reader = PdfReader(io.BytesIO(response.content))\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\\n\"\n",
    "\n",
    "        txt_output_path = os.path.splitext(output_path)[0] + \".txt\"\n",
    "        with open(txt_output_path, \"w\") as f:\n",
    "            f.write(text)\n",
    "        return True\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f65112",
   "metadata": {},
   "outputs": [],
   "source": [
    "fda_url = \"https://www.fda.gov/media/71185/download\"\n",
    "fda_pdf_path = os.path.join(data_paths[\"fda\"], \"fda_diabetes_guidance.pdf\")\n",
    "download_and_extract_text_from_pdf(fda_url, fda_pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ed7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ethics_content = \"\"\"\n",
    "Title: Summary of the Belmont Report Principles for Clinical Research\n",
    "1. Respect for Persons: This principle requires that individuals be treated as autonomous agents and that persons with diminished autonomy are entitled to protection. This translates to robust informed consent process. Inclusion/exclusion criteria must not unduly target or coerce vulnerable populations, such as economically disadvantaged individuals, prisoners, or those with severe cognitive impairments, unless the research is directly intended to benefit that population.\n",
    "2. Beneficence: This principle involves two complementary rules: (1) do not harm and (2) maximize possible benefits and minimize possible harms. The criteria must be designed to select a population that is most likely to benefit and least likely to be harmed by the intervention. The risks to subjects must be reasonable in relation to anticipated benefits.\n",
    "3. Justice: This principle concerns the fairness of distribution of the burdens and benefits of research. The selection of research subjects must be equitable. Criteria should not be designed to exclude certain groups without a sound scientific or safety-related justification. For example, excluding participants based on race, gender or socioeconomic status is unjust unless there is a clear rationale to the drug's mechanism or risk profile.\n",
    "\"\"\"\n",
    "\n",
    "ethics_path = os.path.join(data_paths[\"ethics\"], \"belmont_summary.txt\")\n",
    "\n",
    "with open(ethics_path, \"w\") as f:\n",
    "    f.write(ethics_content)\n",
    "print(f\"Created ethics guideline file: {ethics_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8430b649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb, pandas as pd, os\n",
    "\n",
    "def load_real_mimic_data():\n",
    "    \"\"\"Loads real MIMIC-III CSVs into a persistent DuckDB database file, processing the massive LABEVENTS table efficiently.\"\"\"\n",
    "    print(\"Attempting to load real MIMIC-III data from local CSVs...\")\n",
    "    db_path = os.path.join(data_paths[\"mimic\"], \"mimic3_real.db\")\n",
    "    csv_dir = os.path.join(data_paths[\"mimic\"], \"mimiciii_csvs\")\n",
    "\n",
    "    required_files = {\n",
    "        \"patients\": os.path.join(csv_dir, \"PATIENTS.csv\"),\n",
    "        \"diagnoses\": os.path.join(csv_dir, \"DIAGNOSES_ICD.csv\"),\n",
    "        \"labevents\": os.path.join(csv_dir, \"LABEVENTS.csv\"),\n",
    "    }\n",
    "\n",
    "    missing_files = [path for path in required_files.values() if not os.path.exists(path)]\n",
    "    if missing_files:\n",
    "        print(\"ERROR: The following MIMIC-III files were not found:\")\n",
    "        for f in missing_files: print(f\"- {f}\")\n",
    "        print(\"\\nPlease download them as instructed and place them in the correct directory.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Required files found. Proceeding with database creation.\")\n",
    "    if os.path.exists(db_path):\n",
    "        os.remove(db_path)\n",
    "    con = duckdb.connect(db_path)\n",
    "\n",
    "    print(f\"Loading {required_files['patients']} into DuckDB...\")\n",
    "    con.execute(f\"CREATE TABLE patients AS SELECT SUBJECT_ID, GENDER, DOB, DOD FROM read_csv_auto('{required_files['patients']}')\")\n",
    "    \n",
    "    print(f\"Loading {required_files['diagnoses']} into DuckDB...\")\n",
    "    con.execute(f\"CREATE TABLE diagnoses_icd AS SELECT SUBJECT_ID, ICD9_CODE FROM read_csv_auto('{required_files['diagnoses']}')\")\n",
    "\n",
    "    print(f\"Loading and processing {required_files['labevents']} (this may take several minutes)...\")\n",
    "    # 1. Load the data into a temporary 'staging' table, treating all columns as text ('all_varchar=True').\n",
    "    #    This prevents parsing errors with mixed data types. We also filter for only the lab item IDs we\n",
    "    #    care about (50912 for Creatinine, 50852 for HbAlc) and use a regex to ensure VALUENUM is numeric.\n",
    "\n",
    "    sql = f\"\"\"CREATE TABLE labevents_staging AS\n",
    "                    SELECT SUBJECT_ID, ITEMID, VALUENUM\n",
    "                    FROM read_csv_auto('{required_files['labevents']}', all_varchar=True)\n",
    "                    WHERE ITEMID IN ('50912','50852') AND VALUENUM IS NOT NULL AND VALUENUM ~ '^[0-9]+(\\\\.[0-9]+)?$'\n",
    "                \"\"\"\n",
    "    con.execute(sql)\n",
    "\n",
    "    # 2. Create the final, clean table by selecting from the staging table and casting the columns to their correct numeric types.\n",
    "    con.execute(\"CREATE TABLE labevents AS SELECT SUBJECT_ID, CAST(ITEMID AS INTEGER) AS ITEMID, CAST(VALUENUM AS DOUBLE) AS VALUENUM FROM labevents_staging\")\n",
    "    # 3. Drop the temporary staging table to save space.\n",
    "    con.execute(\"DROP TABLE labevents_staging\")\n",
    "    con.close()\n",
    "    return db_path\n",
    "\n",
    "# Execute the function to build the database\n",
    "try:\n",
    "    db_path = load_real_mimic_data()\n",
    "except Exception as e:\n",
    "    print(f\"Error while building MIMIC DB: {e}\")\n",
    "    db_path = None\n",
    "\n",
    "# IF the database was created successfully, connect to it and inspect the schema and some sample data.\n",
    "if db_path:\n",
    "    print(f\"\\nReal MIMIC-III database created at: {db_path}\")\n",
    "    print(f\"\\nTesting database connection and schema...\")\n",
    "    con = duckdb.connect(db_path)\n",
    "    print(f\"Tables in DB: {con.execute('SHOW TABLES').df()['name'].tolist()}\")\n",
    "    print(\"\\nSample of 'patients' table:\")\n",
    "    print(con.execute(\"SELECT * FROM patients limit 5\").df())\n",
    "    print(\"\\nSample of 'diagnoses_icd table:\")\n",
    "    print(con.execute(\"SELECT * FROM diagnoses_icd LIMIT 5\").df())\n",
    "    con.close()\n",
    "else:\n",
    "    print(\"MIMIC DB not created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cb7f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def create_vector_store(folder_path: str, embedding_model, store_name: str):\n",
    "    \"\"\"Loads all .txt files from a folder, splits them into chunks, and creates an in-memory FAISS vector store.\"\"\"\n",
    "    print(f\"--- Creating {store_name} Vector Store ---\")\n",
    "    loader = DirectoryLoader(folder_path, glob=\"**/*.txt\", loader_cls=TextLoader, show_progress=True)\n",
    "    documents = loader.load()\n",
    "\n",
    "    if not documents:\n",
    "        print(f\"No documents found in {folder_path}, skipping vector store creation.\")\n",
    "        return None\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"Loaded {len(documents)} documents, split into {len(texts)} chunks.\")\n",
    "    print(\"Generating embeddings and indexing into FAISS... (This may take a moment)\")\n",
    "    db = FAISS.from_documents(texts, embedding_model)\n",
    "    print(f\"{store_name} Vector Store created successfully.\")\n",
    "    return db\n",
    "\n",
    "def create_retrievers(embedding_model):\n",
    "    \"\"\"Creates vector store retrievers for all unstructured data sources and consolidates all knowledge stores.\"\"\"\n",
    "    pubmed_db = create_vector_store(data_paths[\"pubmed\"], embedding_model, \"PubMed\")\n",
    "    fda_db = create_vector_store(data_paths[\"fda\"], embedding_model, \"FDA\")\n",
    "    ethics_db = create_vector_store(data_paths[\"ethics\"], embedding_model, \"Ethics\")\n",
    "\n",
    "    return {\n",
    "        \"pubmed_retriever\": pubmed_db.as_retriever(search_kwargs={\"k\": 3}) if pubmed_db else None,\n",
    "        \"fda_retriever\": fda_db.as_retriever(search_kwargs={\"k\": 3}) if fda_db else None,\n",
    "        \"ethics_retriever\": ethics_db.as_retriever(search_kwargs={\"k\": 3}) if ethics_db else None,\n",
    "        \"mimic_db_path\": db_path\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c248a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_stores = create_retrievers(llm_config[\"embedding_model\"])\n",
    "\n",
    "print(\"\\nKnowledge stores and retrievers created successfully.\")\n",
    "for name, store in knowledge_stores.items():\n",
    "    print(f\"{name}: {store}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5b0922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "class GuildSOP(BaseModel):\n",
    "    \"\"\"Standard Operating Procedures for the Trial Design Guild.. This object acts as a dynamic configuraiton for the entire RAG workflow.\"\"\"\n",
    "    planner_prompt: str = Field(description=\"The system prompt for the Planner Agent.\")\n",
    "    researcher_retriever_k: int = Field(description=\"Number of documents for the Medical Researcher to retrieve.\", default=3)\n",
    "    synthesizer_prompt: str = Field(description=\"The system prompt for the Criteria Synthesizer Agent.\")\n",
    "    synthesizer_model: Literal[\"qwen2:7b\", \"llama3.1:8b\"] = Field(description=\"The LLM to use for the Synthesizer.\", default=\"qwen2:7b\")\n",
    "    use_sql_analyst: bool = Field(description=\"Whether to use the Patient Cohort Analyst agent.\", default=True)\n",
    "    use_ethics_specialist: bool = Field(description=\"Whether to use the Ethics Specialist agent.\", default=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3968481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "baseline_sop = GuildSOP(\n",
    "    planner_prompt=\"\"\"You are a master planner for clinical trial design. Your task is to receive a high-level trial concept and break it down into a structured plan with specific sub-tasks for a team of specialists: a Regulatory Specialist, an Ethics Specialist, and a Patient Cohort Analyst. Output a JSON object with a single key 'plan' containing a list of tasks. Each task must have 'agent', 'task_description' keys.\"\"\",\n",
    "    synthesizer_prompt=\"\"\"You are an expert medical writer. Your task is to synthesize the structured findings from all specialist teams into a formal 'Inclusion and Exclusion Criteria' document. Be concise, precise, and adhere strictly to the information provided. Structure your output into two sections: 'Inclusion Criteria' and 'Exclusion Criteria'.\"\"\",\n",
    "    researcher_retriever_k=3,\n",
    "    synthesizer_model=\"qwen2:7b\",\n",
    "    use_sql_analyst=True,\n",
    "    use_ethics_specialist=True\n",
    ")\n",
    "\n",
    "print(\"Baseline GuildSOP (v1.0):\")\n",
    "print(json.dumps(baseline_sop.model_dump(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438d64ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "from pydantic import BaseModel\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class AgentOutput(BaseModel):\n",
    "    \"\"\"A structured output for each agent's findings.\"\"\"\n",
    "    agent_name: str\n",
    "    findings: Any\n",
    "\n",
    "\n",
    "class GuildState(TypedDict):\n",
    "    \"\"\"The state of the Trial Design Guild's workflow, passed between all nodes.\"\"\"\n",
    "    initial_request: str\n",
    "    plan: Optional[Dict[str, Any]]\n",
    "    agent_outputs: List[AgentOutput]\n",
    "    final_criteria: Optional[str]\n",
    "    sop: GuildSOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c0fd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "def planner_agent(state: GuildState) -> GuildState:\n",
    "    \"\"\"Receives the initial request and creates a structured plan for the specialist agents.\"\"\"\n",
    "    print(\"--- EXECUTING PLANNER AGENT ---\")\n",
    "\n",
    "    sop = state[\"sop\"]\n",
    "    llm = llm_config[\"planner\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "{sop.planner_prompt}\n",
    "\n",
    "Trial Concept:\n",
    "{state['initial_request']}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Return a numbered list of execution steps.\n",
    "- Each step must be one short sentence.\n",
    "- Do not include explanations or commentary.\n",
    "\"\"\"\n",
    "\n",
    "    print(f\"Planner Prompt:\\n{prompt}\")\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    raw_text = response.content.strip()\n",
    "    print(f\"Raw Planner Output:\\n{raw_text}\")\n",
    "\n",
    "    # Simple, robust parsing\n",
    "    plan: List[str] = [\n",
    "        line.lstrip(\"0123456789. \").strip()\n",
    "        for line in raw_text.split(\"\\n\")\n",
    "        if line.strip()\n",
    "    ]\n",
    "\n",
    "    print(f\"Parsed Plan:\\n{json.dumps(plan, indent=2)}\")\n",
    "\n",
    "    # Normalize planner output into the structured dict expected downstream\n",
    "    structured_plan = {\"plan\": [{\"agent\": \"Generalist\", \"task_description\": s} for s in plan]}\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"plan\": structured_plan\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80659061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_agent(task_description: str, state: GuildState, retriever_name: str, agent_name: str) -> AgentOutput:\n",
    "    \"\"\"A generic agent function that performs retrieval from a specific vector store based on a task description.\"\"\"\n",
    "    print(f\"--- EXECUTING {agent_name.upper()} ---\")\n",
    "    print(f\"Task: {task_description}\")\n",
    "\n",
    "    retriever = knowledge_stores[retriever_name]\n",
    "\n",
    "    if agent_name == \"Medical Researcher\":\n",
    "        retriever.search_kwargs['k'] = state['sop'].researcher_retriever_k\n",
    "        print(f\"Using k={state['sop'].researcher_retriever_k} for retrieval.\")\n",
    "\n",
    "    retrieval_docs = retriever.invoke(task_description)\n",
    "\n",
    "    findings = \"\\n\\n---\\n\\n\".join([f\"Source: {doc.metadata.get('source', 'N/A')}\\n\\n{doc.page_content}\" for doc in retrieval_docs])\n",
    "    print(f\"Retrieved {len(retrieval_docs)} documents\")\n",
    "    print(f\"Sample finding:\\n{findings[:500]}...\")\n",
    "\n",
    "    return AgentOutput(agent_name, findings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb176668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def patient_cohort_analyst(task_description: str, state: GuildState) -> AgentOutput:\n",
    "    \"\"\"Estimates cohort size by generating and then executing a SQL query against the MIMIC database.\"\"\"\n",
    "    print(\"--- EXECUTING PATIENT COHORT ANALYST ---\")\n",
    "\n",
    "    if not state['sop'].use_sql_analyst:\n",
    "        print(\"SQL Analyst skipped as per SOP.\")\n",
    "        return AgentOutput(agent_name=\"Patient Cohort Analyst\", findings=\"Analysis skipped as per SOP.\")\n",
    "    \n",
    "\n",
    "    con = duckdb.connect(knowledge_stores['mimic_db_path'])\n",
    "    schema_query = \"\"\"\n",
    "    SELECT table_name, column_name, data_type\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema = 'main' ORDER BY table_name, column_name;\n",
    "    \"\"\"\n",
    "    schema = con.execute(schema_query).df()\n",
    "    con.close()\n",
    "\n",
    "    sql_generation_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", f\"You are an expert SQL writer specializing in DuckDB. Your task is to write a single, valid SQL query to count unique patients based on a request. The database contains MIMIC-III patient data with the following schema:\\n{schema.to_string()}\\n\\nIMPORTANT: All column names in your query MUST be uppercase (e.g., SELECT SUBJECT_ID, ICD9_CODE...).\\n\\nKey Mappings:\\m- T2DM (Type 2 Diabetes) corresponds to ICD9_CODE '25000'.\\n-Moderate renal impairment can be estimated by a creatinine lab value (ITEM 50912) where VALUENUM is between 1.5 and 3.0.\\n- Uncontrolled T2D can be estimated by an HbAlc lab value (ITEMID 50852) where VALUENUM is greater than 8.0\"),\n",
    "        (\"human\", \"Please write a SQL query to count the number of unique patients who meet the following criteria: {task}\")\n",
    "    ])\n",
    "\n",
    "    sql_chain = sql_generation_promp | llm_config['sql_coder'] | StrOutputParser()\n",
    "\n",
    "    print(f\"Generating SQL for task: {task_description}\")\n",
    "    sql_query = sql_chain.invoke({\"task\": task_description})\n",
    "    sql_query = sql_query.strip().replace(\"```sql\", \"\").replace(\"```\", \"\")\n",
    "    print(f\"Generate SQL Query:\\n{sql_query}\")\n",
    "    try:\n",
    "        con = duckdb.connect(knowledge_stores['mimic_db_path'])\n",
    "        result = con.execute(sql_query).fetchone()\n",
    "        patient_count = result[0] if result else 0\n",
    "        con.close()\n",
    "\n",
    "        findings = f\"Generated SQL Query:\\n{sql_query}\\n\\nEstimated eligible patient count from the database: {patient_count}.\"\n",
    "        print(f\"Query executed successfully. Estimated patient count: {patient_count}\")\n",
    "    except Exception as e:\n",
    "        findings = f\"Error executing SQL query: {e}. Defaulting to a count of 0.\"\n",
    "        print(f\"Error during query execution: {e}\")\n",
    "    return AgentOutput(agent_name=\"Patient Cohort Analyst\", findings=findings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd7e8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criteria_synthesizer(state: GuildState) -> GuildState:\n",
    "    \"\"\"Synthesizes all the structured findings from the specialist agents into the final criteria document.\"\"\"\n",
    "    print(\"--- EXECUTING CRITERIA SYNTHESIZER ---\")\n",
    "\n",
    "    sop = state['sop']\n",
    "    drafter_llm = ChatOllama(model=sop.synthesizer_model, temperature=0.2)\n",
    "\n",
    "    context = \"\\n\\n---\\n\\n\".join([f\"**{out.agent_name} Findings:**\\n{out.findings}\" for out in state['agent_outputs']])\n",
    "\n",
    "    prompt = f\"{sop.synthesizer_prompt}\\n\\n**Context from Specialist Teams:**\\n{context}\"\n",
    "    print(f\"Synthesizer is using model '{sop.synthesizer_model}'.\")\n",
    "\n",
    "    response = drafter_llm.invoke(prompt)\n",
    "    print(\"Final criteria generated.\")\n",
    "\n",
    "    return {**state, \"final_criteria\":response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05810029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "def specialist_execution_node(state: GuildState) -> GuildState:\n",
    "    \"\"\"This node acts as a dispatcher, executing all specialist tasks defined in the plan.\"\"\"\n",
    "    plan_tasks = state['plan']['plan']\n",
    "    outputs = []\n",
    "\n",
    "    for task in plan_tasks:\n",
    "        agent_name = task['agent']\n",
    "        task_desc = task['task_description']\n",
    "\n",
    "        if \"Regulatory\" in agent_name:\n",
    "            output = retrieval_agent(task_desc, state, \"fda_retriever\", \"Regulatory Specialist\")\n",
    "        elif \"Medical\" in agent_name:\n",
    "            output = retrieval_agent(task_desc, state, \"pubmed_retriever\", \"Medical Researcher\")\n",
    "        elif \"Ethics\" in agent_name and state['sop'].use_ethics_specialist:\n",
    "            output = retrieval_agent(task_desc, state, \"ethics_retriever\", \"Ethics Specialist\")\n",
    "        elif \"Cohort\" in agent_name:\n",
    "            output = patient_cohort_analyst(task_desc, state)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        outputs.append(output)\n",
    "\n",
    "    return {**state, \"agent_outputs\": outputs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca0e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GuildState)\n",
    "\n",
    "workflow.add_node(\"planner\", planner_agent)\n",
    "workflow.add_node(\"execution_specialist\", specialist_execution_node)\n",
    "workflow.add_node(\"synthesizer\", criteria_synthesizer)\n",
    "\n",
    "workflow.set_entry_point(\"planner\")\n",
    "\n",
    "workflow.add_edge(\"planner\", \"execution_specialist\")\n",
    "workflow.add_edge(\"execution_specialist\", \"synthesizer\")\n",
    "workflow.add_edge(\"synthesizer\", END)\n",
    "\n",
    "guild_graph = workflow.compile()\n",
    "print(\"Graph compiled successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d55951",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from IPython.display import Image\n",
    "except ImportError:\n",
    "    print(\"Could not import pygraphviz. Install it to visualize the graph.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bcdd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_request = \"Draft inclusion/exclusion criteria for a Phase II trial of 'Sotagliflozin', a novel SGLT2 inhibitor, for adults with uncontrolled Type 2 Diabetes (HbAlc > 8.0%) and moderate chronic kidney disease (CKD Stage 3).\"\n",
    "\n",
    "print(\"Running the full Guild graph with baseline SOP v1.0...\")\n",
    "graph_input = {\n",
    "    \"initial_request\": test_request,\n",
    "    \"sop\": baseline_sop\n",
    "}\n",
    "final_result = guild_graph.invoke(graph_input)\n",
    "\n",
    "print(\"\\nFinal Guild Output:\")\n",
    "print(\"---------------------\")\n",
    "print(final_result['final_criteria'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dbe535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "class GradedScore(BaseModel):\n",
    "    \"\"\"A Pydantic model to structure the output of our LLM-as-a-Judge evaluators.\"\"\"\n",
    "    score: float = Field(description=\"A score from 0.0 to 1.0\")\n",
    "    reasoning: str = Field(description=\"A brief justification for the score.\")\n",
    "\n",
    "# Evaluator 1: Scientific Rigor (LLM-as-a-Judge)\n",
    "def scientific_rigor_evaluator(generated_criteria: str, pubmed_context: str) -> GradedScore:\n",
    "    \"\"\"Evaluates if the generated criteria are scientifically justified by the provided literature.\"\"\"\n",
    "    evaluator_llm = llm_config['director'].with_structured_output(GradedScore)\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an expert clinical scientist. Evaluate a set of clinical trial criteria based on the provided scientific literature. A score of 1.0 means the criteria are perfectly aligned with and justified by the literature. A score of 0.0 means they contradict or ignore the literature.\"),\n",
    "        (\"human\", \"Evaluate the following criteria:\\n\\n**Generated Criteria:**\\n{criteria}\\n\\n**Supporting Scientific Context:**\\n{context}\")\n",
    "    ])\n",
    "\n",
    "    chain = prompt | evaluator_llm\n",
    "    return chain.invoke({\"criteria\": generated_criteria, \"context\": pubmed_context})\n",
    "\n",
    "# Evaluator 2: Regulatory Compliance (LLM-as-a-Judge)\n",
    "def regulatory_comliance_evaluator(generated_criteria: str, fda_context: str) -> GradedScore:\n",
    "    \"\"\"Evaluates if the generated criteria adhere to the provided FDA guidelines.\"\"\"\n",
    "    evaluator_llm = llm_config['director'].with_structured_output(GradedScore)\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an expert regulatory affairs specialist. Evaluate if a set of clinical trial criteria adheres to the provided FDA guidelines. A score of 1.0 means full compliance.\"),\n",
    "        (\"human\", \"Evaluate the following criteria:\\n\\n**Generated Criteria:**\\n{criteria}\\n\\n**Applicable FDA Guidelines:**\\n{context}\")\n",
    "    ])\n",
    "\n",
    "    chain = prompt | evaluator_llm\n",
    "    return chain.invoke({\"criteria\": generated_criteria, \"context\": fda_context})\n",
    "\n",
    "# Evaluator 3: Ethical Soundness (LLM-as-a-Judge)\n",
    "def ethical_soundness_evaluator(generated_criteria: str, ethics_context: str) -> GradedScore:\n",
    "    \"\"\"Evaluates if the criteria adhere to the core principles of clinical research ethics.\"\"\"\n",
    "    evaluator_llm = llm_config['director'].with_structured_output(GradedScore)\n",
    "    # The persona is now an \"expert on clinical trial ethics\".\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an expert on clinical trial ethics. Evaluate if a set of criteria adheres to the ethical principles provided (summarizing the Belmont Report). A score of 1.0 means the criteria show strong respect for persons, beneficence, and justice.\"),\n",
    "        (\"human\", \"Evaluate the following criteria:\\n\\n**Generated Criteria:**\\n{criteria}\\n\\n**Ethical Principles:**\\n{context}\")\n",
    "    ])\n",
    "    chain = prompt | evaluator_llm\n",
    "    # We use the context from the Ethics Specialist's retriever.\n",
    "    return chain.invoke({\"criteria\": generated_criteria, \"context\": ethics_context})\n",
    "\n",
    "# Evaluator 4: Recruitment Feasibility (Programmatic)\n",
    "def feasibility_evaluator(cohort_analyst_output: AgentOutput) -> GradedScore:\n",
    "    \"\"\"Scores feasibility by parsing the patient count from the SQL Analyst's output and normalizing it.\"\"\"\n",
    "    findings_text = cohort_analyst_output.findings\n",
    "    try:\n",
    "        count_str = findings_text.split(\"database: \")[1].replace(\".\", \"\")\n",
    "        patient_count = int(count_str)\n",
    "    except (IndexError, ValueError):\n",
    "        return GradedScore(score=0.0, reasoning=\"Could not parse patient count from analyst output.\")\n",
    "    \n",
    "    IDEAL_COUNT = 150.0\n",
    "    score = min(1.0, patient_count / IDEAL_COUNT)\n",
    "    reasoning = f\"Estimated {patient_count} eligible patients. Score is normalized against an ideal target of {int(IDEAL_COUNT)}.\"\n",
    "    return GradedScore(score=score, reasoning=reasoning)\n",
    "\n",
    "# Evaluator 5: Operational Simplicity (Programmatic)\n",
    "def simplicity_evaluator(generated_criteria: str) -> GradedScore:\n",
    "    \"\"\"Scores simplicity by penalizing the inclusion of expensive or complex screening tests.\"\"\"\n",
    "    EXPENSIVE_TESTS = [\"mri\", \"genetic sequencing\", \"pet scan\", \"biopsy\", \"echocardiagram\", \"endoscopy\"]\n",
    "    test_count = sum(1 for test in EXPENSIVE_TESTS if test in generated_criteria.lower())\n",
    "\n",
    "    score = max(0.0, 1.0 - (test_count * 0.5))\n",
    "    reasoning = f\"Found {test_count} expensive/complex screening procedures mentioned.\"\n",
    "    return GradedScore(score, reasoning)\n",
    "\n",
    "\n",
    "class EvaluationResult(BaseModel):\n",
    "    \"\"\"A Pydantic model to hold the complete 5D evaluation result.\"\"\"\n",
    "    rigor: GradedScore\n",
    "    compliance: GradedScore\n",
    "    ethics: GradedScore\n",
    "    feasibility: GradedScore\n",
    "    simplicity: GradedScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f410ecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_evaluation(guild_final_state: GuildState) -> EvaluationResult:\n",
    "    \"\"\"Orchestrates the entire evaluation process, calling each of the five specialist evaluators.\"\"\"\n",
    "    print(\"--- RUNNING FULL EVALUATION GAUNTLET ---\")\n",
    "\n",
    "    final_criteria = guild_final_state['final_criteria']\n",
    "    agent_outputs = guild_final_state['agent_outputs']\n",
    "\n",
    "    pubmed_context = next((o.findings for o in agent_outputs if o.agent_name == \"Medical Researcher\"), \"\")\n",
    "    fda_context = next((o.findings for o in agent_outputs if o.agent_name == \"Regulatory Specialist\"), \"\")\n",
    "    ethics_context = next((o.findings for o in agent_outputs if o.agent_name == \"Ethics Specialist\"), \"\")\n",
    "    analyst_output = next((o.findings for o in agent_outputs if o.agent_name == \"Patient Cohort Analyst\"), None)\n",
    "\n",
    "    print(\"Evaluating: Scientific Rigor...\")\n",
    "    rigor = scientific_rigor_evaluator(final_criteria, pubmed_context)\n",
    "    print(\"Evaluating: Regulatory Compliance...\")\n",
    "    compliance = regulatory_comliance_evaluator(final_criteria, fda_context)\n",
    "    print(\"Evaluating: Ethical Soundness...\")\n",
    "    ethics = ethical_soundness_evaluator(final_criteria, ethics_context)\n",
    "    print(\"Evaluating: Recruitment Feasibility...\")\n",
    "    feasibility = feasibility_evaluator(analyst_output) if analyst_output else GradedScore(score=0, reasoning=\"Analyst did not run.\")\n",
    "    print(\"Evaluating: Operational Simplicity...\")\n",
    "    simplicity = simplicity_evaluator(final_criteria)\n",
    "\n",
    "    print(\"--- EVALUATION GAUNTLET COMPLETE ---\")\n",
    "    return EvaluationResult(rigor=rigor, compliance=compliance, ethics=ethics, feasibility=feasibility, simplicity=simplicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b24fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_evaluation_result = run_full_evaluation(final_result)\n",
    "\n",
    "print(\"\\nFull Evaluation Result for Baseline SOP:\")\n",
    "print(json.dumps(baseline_evaluation_result.model_dump(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef46ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SOPGenePool:\n",
    "    \"\"\"A simple class to store and manage a collection of GuildSOPs and their evaluations, acting as our 'gene pool'.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.pool: List[Dict[str, Any]] = []\n",
    "        self.version_counter = 0\n",
    "    \n",
    "    def add(self, sop: GuildSOP, eval_result: EvaluationResult, parent_version: Optional[int] = None):\n",
    "        \"\"\"Adds a new SOP and its evaluation result to the pool.\"\"\"\n",
    "        self.version_counter += 1\n",
    "        entry = {\n",
    "            \"version\": self.version_counter,\n",
    "            \"sop\": sop,\n",
    "            \"evaluation\": eval_result,\n",
    "            \"parent\": parent_version\n",
    "        }\n",
    "        self.pool.append(entry)\n",
    "        print(f\"Added SOP v{self.version_counter} to the gene pool.\")\n",
    "    \n",
    "    def get_latest_entry(self) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"A convenience method to retrieve the most recently added entry.\"\"\"\n",
    "        return self.pool[-1] if self.pool else None\n",
    "\n",
    "\n",
    "class Diagnosis(BaseModel):\n",
    "    \"\"\"A Pydantic model for the structured output of the Diagnostician agent.\"\"\"\n",
    "    primary_weakness: Literal['rigor', 'compliance', 'ethics', 'feasibility', 'simplicity']\n",
    "    root_cause_analysis: str = Field(description=\"A detailed analysis of why the weakness occured, referencing specific scores.\")\n",
    "    recommendation: str = Field(description=\"A high-level recommendation for how to modify the SOP to address the weakness.\")\n",
    "\n",
    "\n",
    "def performance_diagnostician(eval_result: EvaluationResult) -> Diagnosis:\n",
    "    \"\"\"Analyzes the 5D evaluation vector and diagnoses the primary weakness.\"\"\"\n",
    "    print(\"--- EVALUATING PERFORMANCE DIAGNOSTICIAN ---\")\n",
    "    diagnostician_llm = llm_config['director'].with_structured_output(Diagnosis)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a world-class management consultant specializing in process optimization. Your task is to analyze a performance scorecard and identify the single biggest weakness. Then, provide a root cause analysis and a strategic recommendation.\"),\n",
    "        (\"human\", \"Please analyze the following performance evaluation report:\\n\\n{report}\")\n",
    "    ])\n",
    "\n",
    "    chain = prompt | diagnostician_llm\n",
    "    return chain.invoke({\"report\": eval_result.json()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02d024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvolvedSOPs(BaseModel):\n",
    "    \"\"\"A Pydantic container for a list of new, evolved GuildSOPs.\"\"\"\n",
    "    mutations: List[GuildSOP]\n",
    "\n",
    "def sop_architect(diagnosis: Diagnosis, current_sop: GuildSOP) -> EvolvedSOPs:\n",
    "    \"\"\"Takes a diagnosis and the current SOP, and generates a list of new, mutated SOPs to test.\"\"\"\n",
    "    print(\"--- EXECUTING SOP ARCHITECT ---\")\n",
    "    architect_llm = llm_config['director'].with_structured_output(EvolvedSOPs)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", f\"You are an AI process architect. Your job is to modify a process configuration (an SOP) to fix a diagnosed problem. The SOP is a JSON object with this schema: {GuildSOP.model_json_schema()}. You must return a list of 2-3 new, valid SOP JSON objects under the 'mutations' key. Propose diverse and creative mutations. For example, you can change prompts, toggle agents, change retrieval parameters, or even change the model used for a task. Only modify fields relevant to the diagnosis.\"),\n",
    "        (\"human\", \"Here is the current SOP:\\n{current_sop}\\n\\nhere is the performance diagnosis:\\n{diagnosis}\\n\\nBased on the diagnosis, please provide 2-3 new, improved SOPs.\")\n",
    "    ])\n",
    "\n",
    "    chain = prompt | architect_llm\n",
    "    return chain.invoke({\"current_sop\": current_sop.model_dump_json(), \"diagnosis\": diagnosis.model_dump_json()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd225ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evolution_cycle(gene_pool: SOPGenePool, trial_request: str):\n",
    "    \"\"\"Runs one full cycle of diagnosis, mutation, and re-evaluation.\"\"\"\n",
    "    print(\"\\n\"+\"=\"*25+\" STARTING NEW EVOLUTION CYCLE \"+\"=\"*25)\n",
    "\n",
    "    # Step 1: Select the current best SOP to improve upon. For simplicity, we'll just take the latest one added to the pool.\n",
    "    current_best_entry = gene_pool.get_latest_entry()\n",
    "    parent_sop = current_best_entry['sop']\n",
    "    parent_eval = current_best_entry['evaluation']\n",
    "    parent_version = current_best_entry['version']\n",
    "    print(f\"Improving upong SOP v{parent_version}...\")\n",
    "\n",
    "    # Step 2: Diagnose the performance of the parent SOP\n",
    "    diagnosis = performance_diagnostician(parent_eval)\n",
    "    print(f\"Diagnosis complete. Primary Weakness: '{diagnosis.primary_weakness}'. Recommendation: {diagnosis.recommendation}\")\n",
    "\n",
    "    # Step 3: Architect new SOP candidates based on the diagnosis.\n",
    "    new_sop_candidates = sop_architect(diagnosis, parent_sop)\n",
    "    print(f\"Generated {len(new_sop_candidates.mutations)} new SOP candidates.\")\n",
    "\n",
    "    # Step 4: Evaluate each new candidate by running the full Guild graph and the evaluation gauntlet.\n",
    "    for i, candidate_sop in enumerate(new_sop_candidates.mutations):\n",
    "        print(f\"\\n--- Testing SOP candidate {i+1}/{len(new_sop_candidates.mutations)} ---\")\n",
    "        \n",
    "        guild_input = {\"initial_request\": trial_request, \"sop\": candidate_sop}\n",
    "        final_state = guild_graph.invoke(guild_input)\n",
    "\n",
    "        eval_result = run_full_evaluation(final_state)\n",
    "        gene_pool.add(sop=candidate_sop, eval_result=eval_result, parent_version=parent_version)\n",
    "\n",
    "    print(\"\\n\"+\"=\"*25+\" EVOLUTION CYCLE COMPLETE \"+\"=\"*25)\n",
    "\n",
    "gene_pool = SOPGenePool()\n",
    "print(\"Initialized SOP Gene Pool.\")\n",
    "gene_pool.add(sop=baseline_sop, eval_result=baseline_evaluation_result)\n",
    "run_evolution_cycle(gene_pool, test_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbb17c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll iterate through our gene pool and print a formatted summary of each entry's performance.\n",
    "print(\"SOP Gene Pool Evaluation Summary:\")\n",
    "print(\"---------------------------------\")\n",
    "for entry in gene_pool.pool:\n",
    "    v = entry['version']\n",
    "    p = entry['parent']\n",
    "    evals = entry['evaluation']\n",
    "    # Extract the score from each GradedScore object.\n",
    "    r, c, e, f, s = evals.rigor.score, evals.compliance.score, evals.ethics.score, evals.feasibility.score, evals.simplicity.score\n",
    "    parent_str = f\"(Parent)\" if p is None else f\"(Child of v{p})\"\n",
    "    print(f\"SOP v{v:<2} {parent_str:<14}: Rigor={r:.2f}, Compliance={c:.2f}, Ethics={e:.2f}, Feasibility={f:.2f}, Simplicity={s:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7940bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def identify_pareto_front(gene_pool:SOPGenePool) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Identifies the non-dominated solutions (the Pareto Front) in the gene pool.\"\"\"\n",
    "    pareto_front = []\n",
    "    pool_entries = gene_pool.pool\n",
    "\n",
    "    for i, candidate in enumerate(pool_entries):\n",
    "        is_dominated = False\n",
    "        cand_scores = np.array([s['score'] for s in candidate['evaluation'].dict().values()])\n",
    "\n",
    "        for j, other in enumerate(pool_entries):\n",
    "            if i == j: continue\n",
    "\n",
    "            other_scores = np.array(s['score'] for s in other['evaluation'].dict().values())\n",
    "\n",
    "            if np.all(other_scores >= cand_scores) and np.any(other_scores > cand_scores):\n",
    "                is_dominated = True\n",
    "                break\n",
    "\n",
    "        if not is_dominated:\n",
    "            pareto_front.append(candidate)\n",
    "    return pareto_front\n",
    "\n",
    "pareto_sops = identify_pareto_front(gene_pool)\n",
    "\n",
    "print(\"SOPs on the Pareto Front:\")\n",
    "print(\"-------------------------\")\n",
    "for entry in pareto_sops:\n",
    "    v = entry['version']\n",
    "    evals = entry['evaluation']\n",
    "    r, c, e, f, s = evals.rigor.score, evals.compliance.score, evals.ethics.score, evals.feasibility.score, eval.simplicity.score\n",
    "    print(f\"SOP v{v}: Rigor={r:.2f}, Compliance={c:.2f}, Ethics={e:.2f}, Feasibility={f:.2f}, Simplicity={s:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c18791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, pandas as pd\n",
    "\n",
    "def visualize_frontier(pareto_sops):\n",
    "    \"\"\"Creates a 2D scatter plot and a parallel coordinates plot to visualize the Pareto front.\"\"\"\n",
    "    if not pareto_sops:\n",
    "        print(\"No SOPs on the Pareto front to visualize.\")\n",
    "        return\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "    # --- Plot 1: 2D Scatter Plot (Rigor vs. Feasibility) ---\n",
    "    labels = [f\"v{s['version']}\" for s in pareto_sops]\n",
    "    rigor_scores = [s['evaluation'].rigor.score for s in pareto_sops]\n",
    "    feasibility_scores = [s['evaluation'].feasibility.score for s in pareto_sops]\n",
    "\n",
    "    ax1.scatter(rigor_scores, feasibility_scores, s=200, alpha=0.7, c='blue')\n",
    "    for i, txt in enumerate(labels):\n",
    "        ax1.annotate(txt, (rigor_scores[i], feasibility_scores[i]), xytext=(10, -10), textcoords='offset points', fontsize=14)\n",
    "    ax1.set_title('Pareto Frontier: Rigor vs. Feasibility', fontsize=16)\n",
    "    ax1.set_xlabel('Scientific Rigor Score', fontsize=14)\n",
    "    ax1.set_ylabel('Recruitment Feasibility Score', fontsize=14)\n",
    "    ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax1.set_xlim(min(rigor_scores)-0.05, max(rigor_scores)+0.05)\n",
    "    ax1.set_ylim(min(feasibility_scores)-0.1, max(feasibility_scores)+0.1)\n",
    "\n",
    "\n",
    "    # --- Plot 2: Parallel Coordinates Plot for 5D Analysis ---\n",
    "    data = []\n",
    "    for s in pareto_sops:\n",
    "        eval_dict = s['evaluation'].dict()\n",
    "        scores = {k.capitalize(): v['score'] for k, v in eval_dict.items()}\n",
    "        scores['SOP Version'] = f\"v{s['version']}\"\n",
    "        data.append(scores)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # The core plotting function from pandas.\n",
    "    pd.plotting.parallel_coordinates(df, 'SOP Version', colormap=plt.get_cmap(\"viridis\"), ax=ax2, axvlines_kwargs={\"linewidth\": 1, \"color\": \"grey\"})\n",
    "    ax2.set_title('5D Performance Trade-offs on Pareto Front', fontsize=16)\n",
    "    ax2.grid(True, which='major', axis='y', linestyle='--', alpha=0.6)\n",
    "    ax2.set_ylabel('Normalized Score', fontsize=14)\n",
    "    ax2.legend(loc='lower center', bbox_to_anchor=(0.5, -0.15), ncol=len(labels))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_frontier(pareto_sops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bd1e74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
